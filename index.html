<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<style type="text/css">
	table.tableizer-table {
		font-size: 12px;
		border: 1px solid #CCC; 
		font-family: Arial, Helvetica, sans-serif;
	} 
	.tableizer-table td {
		padding: 4px;
		margin: 3px;
		border: 1px solid #CCC;
	}
	.tableizer-table th {
		background-color: #104E8B; 
		color: #FFF;
		font-weight: bold;
	}
</style>

<html>
<head>
	<title>Self-Supervised Auxiliary Feature Learning (SAFL) for Domain Adaptive Semantic Segmentation</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Creative and Descriptive Paper Title." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Self-Supervised Auxiliary Feature Learning (SAFL) for Domain Adaptive Semantic Segmentation</span>
		<table align=center width=600px>
			<table align=center width=600px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="http://tombu.me/">Tom Hao Bu</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://sujaybajracharya.me/">Sujay Bajracharya</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://www.sh8.io">Shun Iwase</a></span>
						</center>
					</td>
				</tr>
			</table>
			<!-- <table align=center width=250px> -->
			<!-- 	<tr> -->
			<!-- 		<td align=center width=120px> -->
			<!-- 			<center> -->
			<!-- 				<span style="font&#45;size:24px"><a href=''>[Paper]</a></span> -->
			<!-- 			</center> -->
			<!-- 		</td> -->
			<!-- 		<td align=center width=120px> -->
			<!-- 			<center> -->
			<!-- 				<span style="font&#45;size:24px"><a href='https://github.com/richzhang/webpage&#45;template'>[GitHub]</a></span><br> -->
			<!-- 			</center> -->
			<!-- 		</td> -->
			<!-- 	</tr> -->
			<!-- </table> -->
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:800px" src="./resources/teaser.png"/>
					</center>
				</td>
			</tr>
			<tr>
				<td width=400px>
					<center>
						<span style="font-size:14px">
							Comparison between a baseline (DeepLabV3) and our proposed methods (SAFL, SAFL w/o Adv Feature, DeepLab + Adv Feature). We show the first three examples from the validation dataset in the Cityscapes dataset.
						</span>
					</center>
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				We propose a novel domain adaptation method, SAFL for semantic segmentation. Assuming we only have an access to ground-truth segmentation maps in a source domain, SAFL learns the features of the source (synthetic) and target (real) domains in a supervised and self-supervised manner respectively. For self-supervised training, we use image reconstruction to learn the features of the target domain without ground-truth labels. Our experimental results show that the self-supervised training via image reconstruction in the target domain helps our method improve the mIoU score in semantic segmentation significantly.
			</td>
		</tr>
	</table>
	<br>

	<hr>
	<center><h1>Introduction</h1></center>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
		Syn2Real is an approach to train a deep neural network (DNN) with only synthetic labeled images and optionally unlabeled real images.
This approach is gaining popularity due to its potential ability to reduce the need of annotating large quantities of real images. It is especially beneficial for pixel-level tasks such as semantic segmentation and change detection, where labeling is the most time-consuming and labor-intensive.
Another merit of Syn2Real is its ability to generate synthetic images diverse in weather and illumination conditions. This advantage, for instance, makes it easier for a trained model to deal with rare events like snow, rain, and fog in the field of autonomous driving. However, Syn2Real has a challenge of poor generalization to real images caused by the domain gap between real and synthetic images. To address this problem, many methods have been proposed in recent years such as unsupervised domain adaptation (DA), unsupervised representation learning, and learning from noisy or pseudo labels. Generally, unsupervised DA and representation learning train the network to make the feature distributions of the target and source domain closer. However, aligning the real (target) domain to the synthetic (source) domain distribution is an ill-posed problem because the real domain is significantly more complex and contains many more objects and nuisances. Learning from noisy labels is the latest approach in domain adaptation and achieves promising results. However, training with pseudo-labels can be unstable and often requires special tuning to avoid degeneration. To achieve stable training while maintaining high accuracy, we propose Self-Supervised Auxiliary Feature Learning (SAFL) --- a novel network architecture which learns domain adaptive features for semantic segmentation through the training of self-supervised image inpainting of unlabeled real images. We hypothesize that the task of self-supervised learning on real images acts as a strong regularizer for the network to perform well in both the source and target domains. To evaluate our method, we plan to conduct experiments of domain adaptive segmentation using the GTAV (synthetic) and Cityscapes (real) datasets.
				</td>
			</tr>
		</center>
	</table>
	<hr>

	<center><h1>Related Work</h1></center>

	<table align=center width=850px>
		<center>
			<tr>
				<td>
		The recent methods for semantic segmentation generally use an encode-decoder architecture like UNet or Deeplab. In this project, our architecture is based on Deeplab. Prior work of semi-supervised semantic segmentation makes use of adversarial learning, image inpainting, as well as, learning features invariant to domain shift.
				</td>
			</tr>
		</center>
	</table>

	<hr>

	<center><h1>Proposed Method (SAFL)</h1></center>

	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/method_diagram.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	
	<center><h2>Architecture</h2></center>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					We show a high-level overview of SAFL in the figure above. The network architecture is really simple. SAFL is composed of four subnetworks: an encoder, a semantic segmentation decoder, an image inpainting decoder, and a discriminator. The encoder is based on the the ResNet-101 backbone and the decoder is based on the DeepLabV3 semantic segmentation architecture. During training, all images are fed into the encoder first, and then the encoded feature is given to two decoders to predict the semantic segmentation or image painting. The discriminator network is used to align the distributions of the features between the synthetic and real images. We plan to perform some ablation studies to show the effectiveness of SAFL: 1) Remove the discriminator, 2) Use 2D image reconstruction instead of image inpainting, 3) Use a different backbone network like ResNext or a Transformer 4) Train using additional in-the-wild city images. We aim to achieve state of the art results with our unified but simple method for adaptive semantic segmentation.
				</td>
			</tr>
		</center>
	</table>

	<!-- <table align=center width=800px> -->
	<!-- 	<br> -->
	<!-- 	<tr><center> -->
	<!-- 		<span style="font&#45;size:28px">&#38;nbsp;<a href='https://github.com/richzhang/webpage&#45;template'>[GitHub]</a> -->
	<!-- 		</center> -->
	<!-- 	</span> -->
	<!-- </table> -->
	<center><h2>Training</h2></center>
	<table align=center width=850px>
		<tr>
			<td width=400px>
				<left>
					<p>In order to solve the task, the model has to implicitly learn about semantics and develop image features that are broadly useful. The encoder learns features from cityscape images in a self-supervised fashion by reconstructing it and the adversarial loss from the discriminator forces the network to learn features that can work in both sim and real domain.</p>

					<p>We formulate this task to adapt from GTA V data to cityscapes data as the following loss function:</p>
					$$L = \lambda_{seg} L_{seg} + \lambda_{rec} L_{rec} + \lambda_{adv} L_{adv}$$
					<br>
					<p>where L_seg is the segmentation loss for synthetic data, L_rec is the reconstruction loss for the real images and L_adv is the adversarial loss from the discriminator. The different losses are weighted with hyperparameters lambda_seg, lambda_rec and lambda_adv. </p>
				</left>
			</td>
		</tr>
		<tr>
			<td width=400px>
				<left>
					<center><h3>Reconstruction</h3></center>
					<p>For the reconstruction loss we use an L2 distance between the reconstructed image and target cityscapes image.</p>
					$$L_{rec} = ||F(x) - x||^2_2$$</br>
				</left>
			</td>
		</tr>
		<tr>
			<td width=400px>
				<left>
					<center><h3>Adversarial loss</h3></center>
					<p>We also add an adversarial loss on the feature space based on Generative Adversarial Networks. The discriminator takes as input image features and tries to classify them as real or synthetic. The goal is to learn features that are similar to real image features so the discriminator is fooled.</p>
					$$L_{adv} = ||D(z) – 1||^2_2$$</br>
					<p>where z is the image features, D is the discriminator.</p>
					<p>This least squares GAN loss is based on [ “Least Squares Generative Adversarial Networks.”] which observed that using binary cross entropy loss for generated images which are very different from real images can lead to vanishing gradients.</p>
				</left>
			</td>
		</tr>
	</table>
	<hr>

	<table align=center width=850px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Experiments</h1></center>
					We train our method on approx. 10% of GTAV dataset and evaluate on cityscapes data. [Dataset stats: num of images, epochs trained for etc.]
				</left>
			</td>
		</tr>
		<tr>
			<td><center><img src="./resources/performance_miou.png"/></center></td>
		</tr>
		<tr>
			<td width=400px>
				<left>
					<center><h2>Quantitative results</h2></center>
					We compare our method with the baseline which learns semantic segmentation on GTA V and naively transfers to cityscapes in the first column. SAFL outperforms this baseline as seen with the second and third column. Adding adversarial loss improves performance over just using reconstruction. However, we found that removing the reconstruction auxiliary output and just doing segmentation with adversarial loss does better (in the 4th and 5th cols). The last column shows the oracle performance which is the deeplab network trained on cityscapes data.
				</left>
			</td>
		</tr>
	</table>

	<br>
	<table class="tableizer-table" align=center>
	<thead><tr class="tableizer-firstrow"><th>Class</th><th>Baseline</th><th>SAFL</th><th>Deeplab + seg adv</th><th>Deeplab + feat adv</th><th>Oracle</th></tr></thead><tbody>
	 <tr><td>road</td><td>12.73</td><td>87.25</td><td>85.12</td><td>91.527</td><td>93.06</td></tr>
	 <tr><td>sidewalk</td><td>19.98</td><td>9.99</td><td>1.11</td><td>15.375</td><td>60.34</td></tr>
	 <tr><td>building</td><td>75.41</td><td>75.64</td><td>76.12</td><td>72.404</td><td>81.57</td></tr>
	 <tr><td>wall</td><td>15.32</td><td>16.85</td><td>15.95</td><td>1.696</td><td>42.85</td></tr>
	 <tr><td>fence</td><td>17.68</td><td>3.74</td><td>12.4</td><td>21.525</td><td>34.05</td></tr>
	 <tr><td>pole</td><td>16.99</td><td>12.6</td><td>17.86</td><td>18.886</td><td>15.97</td></tr>
	 <tr><td>traffic light</td><td>18.1</td><td>14.86</td><td>18.93</td><td>22.642</td><td>29.24</td></tr>
	 <tr><td>traffic sign</td><td>12.65</td><td>3.66</td><td>7.47</td><td>17.379</td><td>36.44</td></tr>
	 <tr><td>vegetation</td><td>77.83</td><td>76.1</td><td>77.27</td><td>81.499</td><td>81.1</td></tr>
	 <tr><td>terrain</td><td>11.43</td><td>27.57</td><td>24.65</td><td>33.558</td><td>45.25</td></tr>
	 <tr><td>sky</td><td>77.74</td><td>69.59</td><td>71.42</td><td>55.217</td><td>82.27</td></tr>
	 <tr><td>person</td><td>31.88</td><td>33.01</td><td>40.42</td><td>35.463</td><td>47.24</td></tr>
	 <tr><td>rider</td><td>0.17</td><td>1.57</td><td>4.99</td><td>23.984</td><td>29.44</td></tr>
	 <tr><td>car</td><td>20.01</td><td>71.88</td><td>79.62</td><td>80.481</td><td>76.93</td></tr>
	 <tr><td>truck</td><td>5.98</td><td>12.49</td><td>26.53</td><td>36.229</td><td>42.83</td></tr>
	 <tr><td>bus</td><td>8.98</td><td>3.23</td><td>19.07</td><td>16.913</td><td>50.64</td></tr>
	 <tr><td>train</td><td>1.37</td><td>11.29</td><td>0.19</td><td>0</td><td>30.36</td></tr>
	 <tr><td>motorcycle</td><td>0.73</td><td>0</td><td>9.63</td><td>20.736</td><td>29.02</td></tr>
	 <tr><td>bicycle</td><td>0.09</td><td>0.72</td><td>0.5</td><td>18.088</td><td>46.68</td></tr>
	</tbody></table>
	<br>
	<table align=center width=850px>
		<tr>
			<td width=400px>
				<left>
					Table 1 shows the per class IOU for the different methods. We also tried output space adversarial loss (deeplab + seg adv) but found that feature space adversarial loss (deeplab + feat adv) had better performance.
				</left>
			</td>
		</tr>
	</table>

	<table align=center width=850px>
		<tr>
			<td width=400px>
				<center><h2>Qualitative results</h2></center>
			</td>
		</tr>
	</table>
	<hr>
	<center><h1>Discussion</h1></center>
	<table align=center width=850px>
		<tr>
			<td width=400px>
				<left>
					We found that adding adversarial learning improves the performance of adaptive segmentation. However, when comparing SAFL with adversarial loss with deeplab with adversarial loss, it seems that reconstruction may not be the best self-supervised task to learn a good representation. Looking at the per class IOUs we see that some classes, such as train, bycycle, are particularly difficult. This is probably due to the classes having few examples in the GTA5 dataset. Another challenge is memory limitation. We have to use smaller batch size and resized images during training to avoid running out of memory which can lead to less stable training and worse per pixel accuracy.
				</left>
			</td>
		</tr>
	</table>
	<hr>
	<br>
	<center><h1>Future Work</h1></center>
	<table align=center width=850px>
		<tr>
			<td width=400px>
				<left>
			We currently use a features from the final layer of the resnet backbone before the ASPP layers for feature space adversarial loss. We can use different intermediate features to see how it affects performance. Also, using different auxillary task than reconstruction could be more suited to this domain adaptation task. For example, we can investigate tasks such as denoising or inpainting. Similarly, we can use contrastive learning methods to learn good representation. Finally, we can use a loss that more directly optimzes the mean IOU with respect to class imbalance. We leave these experiments as future work.
			</td>
		</tr>
	</table>
	<hr>

<br>
</body>
</html>

